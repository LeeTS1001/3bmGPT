{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hidden-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = \"data_dd_all_0418_quarter.json\"\n",
    "\n",
    "# Open the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    # Load the contents of the file\n",
    "    data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "congressional-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "num_examples = len(lines)\n",
    "num_train = int(num_examples * 0.7)  # 70% for training\n",
    "num_val = int(num_examples * 0.2)  # 20% for validation\n",
    "num_test = num_examples - num_train - num_val  # rest for test\n",
    "\n",
    "train_data = lines[:num_train]\n",
    "val_data = lines[num_train:num_train+num_val]\n",
    "test_data = lines[num_train+num_val:]\n",
    "\n",
    "# Save train, validation, and test sets to text files\n",
    "with open(\"train_data/rain.txt\", \"w\") as f:\n",
    "    for example in train_data:\n",
    "        f.write(example + \"\\n\")\n",
    "\n",
    "with open(\"train_data/val.txt\", \"w\") as f:\n",
    "    for example in val_data:\n",
    "        f.write(example + \"\\n\")\n",
    "\n",
    "with open(\"train_data/test.txt\", \"w\") as f:\n",
    "    for example in test_data:\n",
    "        f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "collect-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"train_data/test_0148.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "illegal-chester",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/0418_test/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import  DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sustained-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the custom tokenizer with the Hugging Face GPT2TokenizerFast\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"test_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "indonesian-milton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"pad_token\": \"<pad>\", \"mask_token\": \"<mask>\"})\n",
    "#tokenizer.encode(data_dd_all_0418_quarter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "affecting-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, dataset):\n",
    "    \n",
    "    def encode_with_special_tokens(examples):\n",
    "        bos_token = tokenizer.encode(\"<s>\")\n",
    "        eos_token = tokenizer.encode(\"</s>\")\n",
    "        encoded_sequences = [\n",
    "            bos_token + tokenizer.encode(sequence, truncation=True, max_length=800) + eos_token\n",
    "            for sequence in examples[\"text\"]\n",
    "        ]\n",
    "        return {\"input_ids\": encoded_sequences}\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        encode_with_special_tokens,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "moral-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-be4d965029f64e72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to token_data/text/default-be4d965029f64e72/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 2041.35it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 92.28it/s]\n",
      "                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to token_data/text/default-be4d965029f64e72/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.58it/s]\n",
      "100%|██████████| 4992/4992 [27:26<00:00,  3.03ba/s]  \n",
      "100%|██████████| 1427/1427 [07:51<00:00,  3.03ba/s]\n",
      "100%|██████████| 714/714 [03:55<00:00,  3.04ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "text_datasets = {\n",
    "    \"train\": ['train_data/rain.txt'],\n",
    "    \"eval\": ['train_data/val.txt'],\n",
    "    \"test\": ['train_data/test.txt']\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=text_datasets, cache_dir=\"token_data\")\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenized_dataset = tokenize(tokenizer, dataset)\n",
    "\n",
    "# Saving the tokenized dataset\n",
    "tokenized_dataset.save_to_disk('token_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0125_hazi",
   "language": "python",
   "name": "0125_hazi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
